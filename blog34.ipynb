{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_path = 'weibo_3_moods_train.csv'\n",
    "test_path = 'weibo_3_moods_test.csv'\n",
    "types = {0: '喜悦', 1: '愤怒', 2: '哀伤'}\n",
    "pd_train = pd.read_csv(train_path)\n",
    "pd_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目（总体）：289395\n",
      "测试集数目（总体）：72349\n"
     ]
    }
   ],
   "source": [
    "# 打印训练集行数\n",
    "print('训练集数目（总体）：%d' % pd_train.shape[0])\n",
    "# 打印测试集函数\n",
    "print('测试集数目（总体）：%d' % pd_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#中文分词\n",
    "train_words = []\n",
    "test_words = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "stopwords = [\"[\", \"]\", \"）\", \"（\", \")\", \"(\", \"【\", \"】\", \"！\", \"，\", \"$\",\n",
    "             \"·\", \"？\", \".\", \"、\", \"-\", \"—\", \":\", \"：\", \"《\", \"》\", \"=\",\n",
    "             \"。\", \"…\", \"“\", \"?\", \"”\", \"~\", \" \", \"－\", \"+\", \"\\\\\", \"‘\",\n",
    "             \"～\", \"；\", \"’\", \"...\", \"..\", \"&\", \"#\",  \"....\", \",\",\n",
    "             \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"\n",
    "             \"的\", \"和\", \"之\", \"了\", \"哦\", \"那\", \"一个\",  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289395 289395\n",
      "[1, 1, 0, 0, 3]\n",
      "['最 讨厌 有人 跟 我 说 互相 关注 下 吧 我 想 关注 你 自然 会 关注 我 不想 就 别来 烦 我 姐姐 我 不在乎 粉丝 多少 ', '回复 千年老二 恐韩 不服 不行 广州 亚运会 射箭 女团 决赛 由程明 张云录 祝 珊 珊 组成 的 中国队 通过 两轮 加赛 以 环 的 劣势 不 敌 韩国队 屈居亚军 韩国队 夺冠 ', '就是 嘛 小心 被 跨省 追捕 住 在 中南海 不行 驻京办 无孔不入 我 借 新浪 围脖 这个 平台 友情 提醒 一下 胡主席 一定 要 注意 自己 的 身体 一定 不要 乱发 敏感 信息 一定 不要 辜负 党和人民 的 重托 胡锦涛 主席 在 人民网 开通 微博 这是 多么 振奋人心 的 消息 太 激动 这是 里程碑式 标志性 事件 互联网 将 进入 划时代 崭新 的 时代 ', '翘翘 這 個 很 攪 笑 發 給 你 看看 吧 ', '树洞 啊 树洞 我 难过 死 你 不 保佑 我 也 就算 居然 还 让 我 分到 我 不想 去 的 班级 居然 还是 在 三楼 而且 居然 在 厕所 旁边 我 难过 啊 求 安慰 ']\n"
     ]
    }
   ],
   "source": [
    "for line in range(len(pd_train)):\n",
    "    dict_label = pd_train['label'][line]\n",
    "    dict_content = str(pd_train['content'][line]) #float=>str\n",
    "    #print(dict_label,dict_content)\n",
    "    cut_words = \"\"\n",
    "    data = dict_content.strip(\"\\n\")\n",
    "    data = data.replace(\",\", \"\")    #一定要过滤符号 \",\"否则多列  注意到是英文逗号。csv中 英文逗号表示分列。\n",
    "    seg_list = jieba.cut(data, cut_all=False)\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            cut_words += seg + \" \"\n",
    "    #print(cut_words)\n",
    "    label = dict_label\n",
    "    train_labels.append(label)\n",
    "    train_words.append(cut_words)\n",
    "print(len(train_labels), len(train_words)) #289395 289395\n",
    "print(train_labels[:5])\n",
    "print(train_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72349 72349\n",
      "[3, 0, 0, 0, 0]\n",
      "['星期五 去將 軍 奥翠林 的 分店 拉客 加油 呀 因为 老細 会落 來 睇 場 睡 啦 要 早起 搭車 又系 疲勞 的 一天 開 始 ', '回复 哈哈 谢谢 开心 就 好 大 妹子 好 诗 远山 纵暮霞 近林 横霭 桠 急舟 鳞 波长 慢火 渔 窗花 一起 玩 开心 就 好 呵呵 好 好 拜服 飞霞 相映 远山 分 长堤 稀树 晓烟沉 清风徐来 野村 近 一舟 摇去 半 湖痕 ', '回复 我 是 说 这么 多 花花 是 干 啥 的 不干 啥 拍照 啊 呵呵哈 ', '李 卤味 李 卤味 哈哈哈 作业 作业 星期三 就是 星期 山 ! ! ! 幸好 有 贵人相助 下次 去 吃 李 卤味 好不好 哈哈哈哈 哈哈哈 哈哈哈 ', '我 昨天 做空 的 澳元 仍 在手 今晚 有機 會 在 0.9700 水平 之下 收市 到 時 先行 ']\n"
     ]
    }
   ],
   "source": [
    "for line in range(len(pd_test)):\n",
    "    dict_label = pd_test['label'][line]\n",
    "    dict_content = str(pd_test['content'][line])\n",
    "    cut_words = \"\"\n",
    "    data = dict_content.strip(\"\\n\")\n",
    "    data = data.replace(\",\", \"\")\n",
    "    seg_list = jieba.cut(data, cut_all=False)\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            cut_words += seg + \" \"\n",
    "\n",
    "    #  text = \" \".join(list(jieba.cut(text)))\n",
    "    label = dict_label\n",
    "    test_labels.append(label)\n",
    "    test_words.append(cut_words)\n",
    "print(len(test_labels),len(test_words))\n",
    "print(test_labels[:5])\n",
    "print(test_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7089)\t0.2653697319158461\n",
      "  (0, 6766)\t0.271911936824012\n",
      "  (0, 6394)\t0.20697302118474042\n",
      "  (0, 4899)\t0.22226864932059226\n",
      "  (0, 3100)\t0.2545146290153176\n",
      "  (0, 2735)\t0.23398051065790837\n",
      "  (0, 1575)\t0.611410034200445\n",
      "  (0, 1042)\t0.31248674521223585\n",
      "  (0, 726)\t0.23602128819008097\n",
      "  (0, 697)\t0.3416149878776036\n",
      "  (1, 7632)\t0.2746396605549873\n",
      "  (1, 6470)\t0.36056107542458826\n",
      "  (1, 3608)\t0.25465546388338434\n",
      "  (1, 2959)\t0.3615794555086064\n",
      "  (1, 2535)\t0.1596560807002662\n",
      "  (1, 1660)\t0.3355559683193786\n",
      "  (1, 1059)\t0.33200730583563587\n",
      "  (1, 880)\t0.36297521893155027\n",
      "  (1, 767)\t0.26485442236603435\n",
      "  (1, 741)\t0.3856994452867779\n",
      "  (2, 7560)\t0.17293200707162557\n",
      "  (2, 7531)\t0.26275998506317266\n",
      "  (2, 7498)\t0.09445359768986004\n",
      "  (2, 7380)\t0.16149540897173548\n",
      "  (2, 6757)\t0.09845337874080341\n",
      "  :\t:\n",
      "  (2, 3363)\t0.16818638259815935\n",
      "  (2, 2725)\t0.1729879609810629\n",
      "  (2, 2566)\t0.13907365267846508\n",
      "  (2, 2073)\t0.20220188173164827\n",
      "  (2, 1388)\t0.16662941040878823\n",
      "  (2, 1121)\t0.2298207531081246\n",
      "  (2, 1044)\t0.19226396680930297\n",
      "  (2, 1018)\t0.1633479675609198\n",
      "  (2, 923)\t0.2005626245985215\n",
      "  (2, 768)\t0.2384974168303086\n",
      "  (2, 767)\t0.15991180918580095\n",
      "  (2, 419)\t0.3728197058906155\n",
      "  (2, 353)\t0.12085441948955407\n",
      "  (3, 6075)\t1.0\n",
      "  (4, 7907)\t0.47080958508864323\n",
      "  (4, 7491)\t0.13654463688147045\n",
      "  (4, 6648)\t0.19350359781756976\n",
      "  (4, 4632)\t0.22540068571761696\n",
      "  (4, 3437)\t0.5555310420956462\n",
      "  (4, 3413)\t0.2226197482846279\n",
      "  (4, 3189)\t0.25028280565874095\n",
      "  (4, 2039)\t0.24203678321777744\n",
      "  (4, 1366)\t0.24276834308846446\n",
      "  (4, 726)\t0.19369835749839118\n",
      "  (4, 586)\t0.30917904980246\n",
      "(361744, 8194)\n"
     ]
    }
   ],
   "source": [
    "#TFIDF计算\n",
    "#将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "vectorizer = CountVectorizer(min_df=100)   #MemoryError控制参数\n",
    "# vectorizer = TfidfVectorizer(min_df=100)\n",
    "#该类会统计每个词语的tf-idf权值\n",
    "transformer = TfidfTransformer()\n",
    "# tfidf = vectorizer.fit_transform(train_words+test_words)\n",
    "#第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "\n",
    "# 合并在一起进行了tf-idf\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(train_words+test_words))\n",
    "print(tfidf[:5])\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '10000', '10086']\n",
      "单词数量: 8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#获取词袋模型中的所有词语\n",
    "word = vectorizer.get_feature_names()\n",
    "print(word[:15])\n",
    "print(\"单词数量:\", len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361744, 8194)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#将tf-idf矩阵抽取 元素w[i][j]表示j词在i类文本中的tf-idf权重\n",
    "X = coo_matrix(tfidf, dtype=np.float32).toarray()  #稀疏矩阵\n",
    "print(X.shape)\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X[:len(train_labels)]\n",
    "X_test = X[len(train_labels):]\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分类模型\n",
    "clf = MultinomialNB()\n",
    "#clf = svm.LinearSVC()\n",
    "#clf = LogisticRegression(solver='liblinear')\n",
    "#clf = RandomForestClassifier(n_estimators=10)\n",
    "#clf = neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "#clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型的准确度:0.575875271254613\n"
     ]
    }
   ],
   "source": [
    "print('模型的准确度:{}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pre = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5979    0.9793    0.7425     39912\n",
      "           1     0.5274    0.1340    0.2137     10402\n",
      "           2     0.2783    0.0592    0.0976     10933\n",
      "           3     0.2678    0.0484    0.0819     11102\n",
      "\n",
      "    accuracy                         0.5759     72349\n",
      "   macro avg     0.4179    0.3052    0.2839     72349\n",
      "weighted avg     0.4888    0.5759    0.4676     72349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pre, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 0: 喜悦, 1: 愤怒, 2: 厌恶, 3: 低落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}